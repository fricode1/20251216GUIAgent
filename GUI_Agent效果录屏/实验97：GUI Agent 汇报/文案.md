# GUI Agent 在公安业务流程自动化中的应用

## 背景与需求

![alt text](背景与需求.png)

![alt text](背景与需求2.png)

## 流程自动化

- 目前主要依赖RPA/浏览器自动化，规则驱动。
- 发展趋势：基于大模型的 GUI Agent，数据驱动。

### RPA

流程自动化的典型产品形态为 RPA (Robotic Process Automation)，通过模拟人类在计算机界面上的操作逻辑，在不改变企业现有 IT 架构的前提下，实现跨系统、跨平台的业务衔接，能够极大地提升数据流转的效率与精准度。

| 时间   | 里程碑         | 说明 |
| ------ | ------------ | -------- |
| 2005年 | UiPath       | UiPath公司成立。全球RPA行业领军企业，提供自动化平台。帮助企业处理重复性办公任务。市值70亿美元。 |
| 2012年 | RPA 术语提出 | Prism 公司正式提出RPA术语。该行业进入黄金期，大量金融企业使用RPA系统处理银行账单与表格。 |
| 2019年 | 国产 RPA     | 国产RPA产品影刀RPA诞生。广泛应用于电商行业，完成自动上架商品、批量抓取竞品价格等任务。 |

### 浏览器自动化

RPA软件的核心技术之一是浏览器自动化技术，用于模拟人类用户在Web环境下的交互行为，实现复杂业务流程的自动化处理。

| 时间   | 里程碑       | 说明 |
| ------ | ---------- | -------- |
| 2004年 | Selenium   | 成立于2004年，老牌Web自动化工具，目前网页自动化测试及任务处理领域最权威的全球行业标准。 |
| 2017年 | Puppeteer  | 2017年由谷歌推出。基于Chrome DevTools Protocol (CDP)，能够以极高的效率和稳定性控制 Chrome 浏览器。 |
| 2020年 | Playwright | 2020年，由微软发布，由原 Puppeteer 团队打造，支持跨浏览器的统一自动化，解决多窗口、异步加载等待等痛点。 |

### GUI Agent

定义：泛指能够像人类一样通过视觉感知图形界面，并执行点击、输入等交互行为的 AI Agent。

![alt text](GUI_Agent介绍.png)

### 与传统自动化（RPA/浏览器自动化脚本）的区别

| 特性         | 传统自动化 (RPA/Selenium)                         | GUI Agent (AI)                                       |
| :----------- | :------------------------------------------------ | :--------------------------------------------------- |
| 依赖基础     | 依赖底层代码（HTML ID, DOM 结构）或固定坐标       | 依赖视觉图像，像人眼一样看                           |
| 适应性       | 极差。一旦 UI 改版或按钮位置移动，脚本就会报错    | 强。按钮换了位置或颜色，AI 依然认得那是"提交"按钮   |
| 通用性       | 专用的。针对特定软件编写特定脚本                  | 通用的。同一个 Agent 可以学着买火车票，也可以学着发邮件 |
| 指令理解     | 只能执行预设的死命令                              | 可以理解自然语言指令（模糊指令）                     |
| 处理异常     | 遇到未预设的弹窗通常会崩溃                        | 可以根据弹窗内容尝试解决或绕过                       |

#### 适用场景

RPA/浏览器自动化：
- 需求已预先明确的任务、定义明确的任务
- 需要7*24运行的重复性任务
- 容错率极低的任务

GUI Agent：
- 未预定义的任务
- 持续收集新的交互数据，可以对模型进行微调或进一步训练

### 发展现状

#### Anthropic

Anthropic 于 2024 年 10 月发布了闭源 GUI Agent （公测）产品，通过API调用。——按照用户的命令在电脑屏幕上移动光标、点击相关位置，并通过虚拟键盘输入信息，模拟人们与自己电脑的交互方式。

Claude 3.5 Sonnet 是首个在公开测试阶段提供计算机使用功能的 AI 模型。目前，该功能仍处于实验阶段，有时会显得繁琐且容易出错。当时，在 OSWorld 测试集上的准确率为 14.9%。

![alt text](anthropic_20241022.png)

#### OpenAI

OpenAI 于 2025 年 1 月发布了闭源 GUI Agent，像人类一样与图形用户界面 (GUI)——人们在屏幕上看到的按钮、菜单和文本字段——进行交互，无需使用操作系统或网络特定的 API。为研究预览版，仅供美国的 Pro 用户使用。 

在 OSWorld 测试集上的准确率为 38.1%。

![alt text](openai_20250123.png)

#### Google

谷歌于 2025 年 10 月发布了 GUI Agent，这是基于 Gemini 2.5 Pro 构建的专用模型。该模型主要针对网页浏览器进行了优化，但也展现出在移动 UI 控制任务方面的巨大潜力。目前，它尚未针对桌面操作系统级别的控制进行优化。

![alt text](google_20251007.png)

#### 智谱

智谱于 2025 年 12 月发布了 Open-AutoGLM，这是专用于手机的流程自动化智能体。

![alt text](zai_20251208.png)

| 产品/项目              | 开发方/机构     | 地址                                                                 | Stars  | 发布时间       | 备注                 |
|------------------------|-----------------|----------------------------------------------------------------------|--------|----------------|----------------------|
| Claude Computer Use    | Anthropic       | [platform.claude.com](https://platform.claude.com/docs/en/agents-and-tools/tool-use/computer-use) | 闭源   | 2024年10月     | 最先提出           |
| Operator               | OpenAI          | [openai.com](https://openai.com/zh-Hans-CN/index/introducing-operator/) | 闭源   | 2025年1月23日  | 性能第一梯队         |
| UFO²                   | Microsoft       | [github.com/microsoft/UFO](https://github.com/microsoft/UFO)         | 7.8k   | 2025年5月6日   | Win系统深度集成      |
| UI-TARS-desktop        | ByteDance       | [github.com/bytedance/UI-TARS-desktop](https://github.com/bytedance/UI-TARS-desktop) | 19.8k  | 2025年1月23日  | 原生GUI桌面应用      |
| Open Interpreter       | Open Interpreter| [github.com/openinterpreter/open-interpreter](https://github.com/openinterpreter/open-interpreter) | 61k    | 2023年         | star数最高           |
| self-operating-computer| OthersideAI     | [github.com/OthersideAI/self-operating-computer](https://github.com/OthersideAI/self-operating-computer) | 10k    | 2023年         | 多模态操作代理       |
| OS-Copilot             | 上海AI Lab      | [github.com/OS-Copilot/OS-Copilot](https://github.com/OS-Copilot/OS-Copilot) | 1.7k   | 2024年2月      | 通用OS代理框架       |
| ShowUI                 | 新加坡国立大学  | [github.com/showlab/ShowUI](https://github.com/showlab/ShowUI)       | 1.6k   | 2024年11月     | UI视觉理解           |
| Cradle                 | 智源研究院      | [github.com/BAAI-Agents/Cradle](https://github.com/BAAI-Agents/Cradle) | 2.4k   | 2024年3月      | 通用控制框架         |
| OpenCUA                | 香港大学        | [github.com/xlang-ai/OpenCUA](https://github.com/xlang-ai/OpenCUA)   | 594    | 2025年8月      | 开源CUA实现          |
| Agent-S                | Simular AI      | [github.com/simular-ai/Agent-S](https://github.com/simular-ai/Agent-S) | 8.6k   | 2025年10月     | 搜索与学习机制       |
| AppAgent               | 腾讯            | [github.com/TencentQQGYLab/AppAgent](https://github.com/TencentQQGYLab/AppAgent) | 6.3k   | 2023年12月     | 移动端多模态         |
| MobileAgent            | 阿里            | [github.com/X-PLUG/MobileAgent](https://github.com/X-PLUG/MobileAgent) | 6.6k   | 2025年8月      | 移动端多模态         |
| AutoGLM                | 智谱华章        | [github.com/zai-org/Open-AutoGLM](https://github.com/zai-org/Open-AutoGLM) | 2.2k   | 2025年12月9日  | 移动端多模态         |
| Fara-7B                | 微软        | [github.com/microsoft/fara](https://github.com/microsoft/fara) | 3.5k   | 2025年12月24日  | Web端多模态         |

### 各 GUI Agent 的测评结果

![alt text](image-1.png)

## 工作原理

根据用户的指令，GUI Agent 通过一个迭代循环将感知、推理和操作整合在一起：

- 感知：计算机的屏幕截图被添加到模型的上下文中，提供计算机当前状态的视觉快照。 
- 思考：GUI Agent 对下一步进行推理，同时考虑当前和过去的屏幕截图和操作。这种推理过程可使模型评估其观察结果、跟踪中间步骤并进行动态调整，从而提高任务性能。
- 行动：Agent 执行点击、滚动或键入等操作，直到确定任务已完成或需要用户输入。虽然它能自动处理大多数步骤，但对于敏感操作，如输入登录信息或回复验证码表单，GUI Agent 会寻求用户确认。

![alt text](image-2.png)

## 任务定义

给定用户的初始自然语言查询 $q_0$，GUI Agent 会根据环境状态，以多步骤的方式逐一输出动作，直到输出停止动作为止。

一条轨迹中的单个步骤 $t$ 包含三部分：
- 来自网页环境的观测（$o_t$​）
- 对当前状态进行反思、并决定下一步该做什么的思考 / 思维链（$r_t$​）
- 要执行的下一个动作（$a_t$）

一个任务对应的完整轨迹为：

$$\mathcal{T} = (q_0, \{o_0, r_0, a_0\}, \cdots, \{o_T, r_T, a_T\})$$

GUI Agent 对于步骤 $t$ 的预测：

$$P(r_t, a_t | q_0, \{o_0, r_0, a_0\}, \cdots, \{o_{t-1}, r_{t-1}, a_{t-1}\})$$

## 感知

### 基于 DOM 的

![alt text](DOM内容展示.png)

早期的 GUI Agent 受限于 LLM 只能处理文本输入的局限性。因此，这些代理依赖于将GUI页面转换为结构化的文本表示，例如HTML、可访问性树或文档对象模型（DOM）。

然而，DOM的缺点是内容过分冗余。比如，12306.cn主页的DOM文本消耗高达90万tokens。即使对DOM内容进行过滤裁剪，也需要消耗20万tokens。相比而言，一张截图仅消耗1000 tokens左右。

此外，DOM是不完备的。它不是网页的最终呈现，还受到css文件、业务逻辑js代码的很大影响。

比如，我要收集操作轨迹的训练数据。如果依赖截图，完全可以在互联网电脑上收集截图及标注员的操作，拷贝到内网标注平台进行标注。但是如果收集DOM数据，缺少了CSS文件及业务逻辑代码文件，无法在其他设备上还原网页的呈现内容。

在 2025 年的一篇博客中，从技术视角分析了使用 DOM 作为感知方式的缺点。

![alt text](DOM的缺点.png)

### 基于屏幕截图的

难点：GUI图像的信息密度和结构性通常比通用场景图像更高，往往包含数百个排列成复杂布局的元素。模型不仅要识别单个元素，还要理解它们的空间关系和功能交互。此外，GUI图像中的许多元素都很小（例如，在 $1920 \times 1080$ 的屏幕截图中的一个 $10 \times 10$ 像素的图标），这使得准确感知和定位这些元素变得困难。

为了解决这一问题，我们需要构建专门的GUI数据集和训练任务，对基线模型进行微调，从而提高大模型对于屏幕截图中元素感知与定位的准确性。

## 思考与行动

大模型将屏幕截图作为观测输入，首先输出思考内容，例如网页内容或当前状态，以及下一步要采取的行动。然后，根据这些思考内容，模型输出一个以工具调用形式表示的行动。

![alt text](image-5.png)

例如，用户希望 GUI Agent 自动执行任务：在 GitHub 网站上购买 Copilot 增值服务。

大模型首先观察到的是浏览器的初始界面。其思考内容是：应该导航至 GitHub Copilot 产品购买界面，然后执行对应动作。

接下来，大模型观察到已经进入到了指定页面，其思考内容是：应该点击“查看订阅计划及价格”链接。执行的动作为，点击该链接对应的坐标。

GUI Agent 能够执行的全部动作列举如下：

| 动作       | 描述                                   |
| :--------- | :------------------------------------- |
| 按键       | 按指定顺序按下按键（例如 *CTRL+C*）。  |
| 输入       | 在坐标 $(x, y)$ 处输入字符串。         |
| 移动鼠标   | 移动光标悬停在坐标 $(x, y)$ 上。       |
| 左键单击   | 在坐标 $(x, y)$ 处单击鼠标左键。       |
| 滚动       | 滚动鼠标滚轮。                         |
| 访问链接   | 访问指定的 URL。                       |
| 网络搜索   | 使用指定查询进行网络搜索。             |
| 历史回退   | 返回上一页。                           |
| 记忆       | 记忆信息以供将来参考。                 |
| 等待       | 等待指定的秒数。                       |
| 终止       | 结束当前任务。                         |

## 记忆

GUI Agent 能够存储在后续轨迹中可能有用的重要信息。当完成任务所需的关键信息位于不同页面时（例如，比较某一商品在不同零售网站上的价格），记忆功能不可或缺。

# 训练数据集构建与模型训练

未针对流程自动化任务训练的通用模型存在如下缺点：
- 屏幕截图属于领域性很强的图像，且要定位的元素很小，通用模型表现不佳。
- 完成流程自动化任务需要领域专门知识。比如，如果大模型不知道 GitHub Copilot 的产品购买页面对应的网址，便无法完整相关流程自动化任务。

为此，需要设计多种训练任务，通过对通用模型的微调，实现流程自动化的能力。

#### 操作轨迹数据集

$$\mathcal{T} = (q_0, \{o_0, r_0, a_0\}, \cdots, \{o_T, r_T, a_T\})$$

$q_0$: 在 GitHub 网站上购买 Copilot 增值服务。

![alt text](step1.png)
![alt text](step2.png)
![alt text](step3.png)
![alt text](step4.png)
![alt text](step5.png)

训练方式：将每条轨迹的每一步单独作为一个训练样本，把截至当前步骤的历史观测与动作作为模型输入。

我们采用 Qwen2.5-VL 的定位规范，并预测绝对坐标。我们使用标准的交叉熵损失，所有输出都是模型词表中的 token，包括坐标。

#### UI问答任务

一条训练数据包括三部分：
- 截图
- 问题
- 回答

![alt text](image-3.png)

问题：门票的配送日期是？

回答：2025 年 3 月 28 日

#### UI定位任务

一条训练数据包括三部分：
- 截图
- 问题
- 回答

![alt text](image-4.png)

问题：点击衣服的 XL 尺寸

回答：click(1189, 252)

### 训练细节

我们使用 Qwen2.5-VL-7B 作为基座模型，并在其基础上进行监督微调（SFT）。

我们使用标准的交叉熵损失，所有输出都是模型词表中的 token，包括坐标。

把每一步单独切成一条训练数据。

对于轨迹数据，我们将每条轨迹的每一步单独作为一个训练样本，把截至当前步骤的历史观测与动作作为模型输入。

- 优化器：AdamW，其参数设置为 $\beta_1 = 0.9, \beta_2 = 0.95$
- 学习率：在训练步数的前 10% 阶段采用余弦学习率预热策略；预热结束后，初始学习率设置为 $5\times 10^{-6}$
- 梯度裁剪：最大阈值设为 1
- 批次大小 = 128
- 参数精度：BF16

## 基线模型的选择

![alt text](fara7b.png)

微软与2025年底提出的 Fara-7B，以 Qwen2.5-VL-7B 作为基座模型，并在其基础上进行监督微调（SFT），微调样本量为 180 万。

![alt text](fara7b训练数据.png)

训练对于模型处理GUI自动化任务具有关键作用。

与 Fara-7B 类似的模型是 UI-Tars-1.5-7B。两者性能接近，但Fara-7B仅使用监督微调（SFT），而UI-TARS-1.5-7B则采用了强化学习（RL）。SFT是进一步微调的更经济的选择。

![alt text](训练的作用.png)

## 公安领域的模型增量微调

为提升 GUI Agent 在垂直领域的任务成功率，需构建针对公安业务场景的专用数据集，通过监督微调 (SFT) 引导模型学习特定系统的 UI 布局逻辑与业务流程。

首先，基于实际警务需求定义典型任务基准 (Benchmarks)，例如：人员身份信息核查、车辆轨迹溯源等交互任务。随后，利用基线模型自动化执行任务，评估其在公安业务系统上的初始泛化能力。

建立量化的评价体系，计算在benchmark上的任务成功率 (SR)。识别模型在视觉定位、意图推理或长序列决策中的性能瓶颈。

针对模型失效的任务，由标注员进行规范化操作，录制纠正的操作轨迹。该过程不仅捕捉屏幕截图与点击坐标，还需通过思维链（CoT）标注，显式记录每步操作背后的思考过程，从而构建高质量的专家演示数据集。

利用 SFT（监督微调）技术，引导模型学习公安业务系统的特有交互模式。
