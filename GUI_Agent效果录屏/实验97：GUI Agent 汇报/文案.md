# GUI Agent 在公安业务流程自动化中的应用

## 背景与需求

![alt text](背景与需求.png)

![alt text](背景与需求2.png)

## 流程自动化

- 目前主要依赖RPA/浏览器自动化，规则驱动。
- 发展趋势：基于大模型的 GUI Agent，数据驱动。

### RPA

流程自动化的典型产品形态为 RPA (Robotic Process Automation)，通过模拟人类在计算机界面上的操作逻辑，在不改变企业现有 IT 架构的前提下，实现跨系统、跨平台的业务衔接，能够极大地提升数据流转的效率与精准度。

#### UiPath

2005年，UiPath公司成立。全球RPA行业领军企业，提供自动化平台。帮助企业处理重复性办公任务。市值70亿美元。

#### RPA 术语提出

2012年，Prism 公司正式提出RPA术语。该行业进入黄金期，大量金融企业使用RPA系统处理银行账单与表格。

#### 国产 RPA

2019年，国产RPA产品影刀RPA诞生。广泛应用于电商行业，完成自动上架上品、批量抓取竞品价格等任务。

### 浏览器自动化

RPA软件的核心技术之一是浏览器自动化技术，用于模拟人类用户在Web环境下的交互行为，实现复杂业务流程的自动化处理。

#### Selenium

成立于2004年，老牌Web自动化工具，目前网页自动化测试及任务处理领域最权威的全球行业标准。

#### Puppeteer

2017年由谷歌推出。基于Chrome DevTools Protocol (CDP)，能够以极高的效率和稳定性控制 Chrome 浏览器。

#### Playwright

由微软发布，由原 Puppeteer 团队打造，支持跨浏览器的统一自动化，解决多窗口、异步加载等待等痛点。

### GUI Agent

定义：泛指能够像人类一样通过视觉感知图形界面，并执行点击、输入等交互行为的 AI Agent。

![alt text](image.png)

### 与传统自动化（RPA/浏览器自动化脚本）的区别

| 特性         | 传统自动化 (RPA/Selenium)                         | GUI Agent (AI)                                       |
| :----------- | :------------------------------------------------ | :--------------------------------------------------- |
| 依赖基础     | 依赖底层代码（HTML ID, DOM 结构）或固定坐标       | 依赖视觉图像，像人眼一样看                           |
| 适应性       | 极差。一旦 UI 改版或按钮位置移动，脚本就会报错    | 强。按钮换了位置或颜色，AI 依然认得那是"提交"按钮   |
| 通用性       | 专用的。针对特定软件编写特定脚本                  | 通用的。同一个 Agent 可以学着用 Excel，也可以学着用 Photoshop |
| 指令理解     | 只能执行预设的死命令                              | 可以理解自然语言指令（模糊指令）                     |
| 处理异常     | 遇到未预设的弹窗通常会崩溃                        | 可以根据弹窗内容尝试解决或绕过                       |

#### 适用场景

RPA/浏览器自动化：
- 需求已预先明确的任务、定义明确的任务
- 需要7*24运行的重复性任务
- 容错率极低的任务

GUI Agent：
- 未预定义的任务
- 持续收集新的交互数据，可以对模型进行微调或进一步训练

### 发展现状

| 产品/项目              | 开发方/机构     | 地址                                                                 | Stars  | 发布时间       | 备注                 |
|------------------------|-----------------|----------------------------------------------------------------------|--------|----------------|----------------------|
| Claude Computer Use    | Anthropic       | [platform.claude.com](https://platform.claude.com/docs/en/agents-and-tools/tool-use/computer-use) | 闭源   | 2024年10月     | 性能第一梯队           |
| Operator               | OpenAI          | [openai.com](https://openai.com/zh-Hans-CN/index/introducing-operator/) | 闭源   | 2025年1月23日  | 性能第一梯队         |
| UFO²                   | Microsoft       | [github.com/microsoft/UFO](https://github.com/microsoft/UFO)         | 7.8k   | 2025年5月6日   | Win系统深度集成      |
| UI-TARS-desktop        | ByteDance       | [github.com/bytedance/UI-TARS-desktop](https://github.com/bytedance/UI-TARS-desktop) | 19.8k  | 2025年1月23日  | 原生GUI桌面应用      |
| Open Interpreter       | Open Interpreter| [github.com/openinterpreter/open-interpreter](https://github.com/openinterpreter/open-interpreter) | 61k    | 2023年         | star数最高           |
| self-operating-computer| OthersideAI     | [github.com/OthersideAI/self-operating-computer](https://github.com/OthersideAI/self-operating-computer) | 10k    | 2023年         | 多模态操作代理       |
| OS-Copilot             | 上海AI Lab      | [github.com/OS-Copilot/OS-Copilot](https://github.com/OS-Copilot/OS-Copilot) | 1.7k   | 2024年2月      | 通用OS代理框架       |
| ShowUI                 | 新加坡国立大学  | [github.com/showlab/ShowUI](https://github.com/showlab/ShowUI)       | 1.6k   | 2024年11月     | UI视觉理解           |
| Cradle                 | 智源研究院      | [github.com/BAAI-Agents/Cradle](https://github.com/BAAI-Agents/Cradle) | 2.4k   | 2024年3月      | 通用控制框架         |
| OpenCUA                | 香港大学        | [github.com/xlang-ai/OpenCUA](https://github.com/xlang-ai/OpenCUA)   | 594    | 2025年8月      | 开源CUA实现          |
| Agent-S                | Simular AI      | [github.com/simular-ai/Agent-S](https://github.com/simular-ai/Agent-S) | 8.6k   | 2025年10月     | 搜索与学习机制       |
| AppAgent               | 腾讯            | [github.com/TencentQQGYLab/AppAgent](https://github.com/TencentQQGYLab/AppAgent) | 6.3k   | 2023年12月     | 移动端多模态         |
| MobileAgent            | 阿里            | [github.com/X-PLUG/MobileAgent](https://github.com/X-PLUG/MobileAgent) | 6.6k   | 2025年8月      | 移动端多模态         |
| AutoGLM                | 智谱华章        | [github.com/zai-org/Open-AutoGLM](https://github.com/zai-org/Open-AutoGLM) | 2.2k   | 2025年12月9日  | 移动端多模态         |
| Fara-7B                | 微软        | [github.com/microsoft/fara](https://github.com/microsoft/fara) | 3.5k   | 2025年12月24日  | Web端多模态         |

### 各 GUI Agent 的测评结果

![alt text](image-1.png)

## GUI Agent 原理

![alt text](image-2.png)

## 任务定义

给定用户的初始自然语言查询 $q_0$，GUI Agent 会根据环境状态，以多步骤的方式逐一输出动作，直到输出停止动作为止。

一条轨迹中的单个步骤 $t$ 包含三部分：
- 来自网页环境的观测（$o_t$​）
- 对当前状态进行反思、并决定下一步该做什么的思考 / 思维链（$r_t$​）
- 要执行的下一个动作（$a_t$）

一个任务对应的完整轨迹为：

$$\Tau = (q_0, \{o_0, r_0, a_0\}, \cdots, \{o_T, r_T, a_T\})$$

GUI Agent 对于步骤 $t$ 的预测：

$$P(r_t, a_t | q_0, \{o_0, r_0, a_0\}, \cdots, \{o_{t-1}, r_{t-1}, a_{t-1}\})$$

## 感知

### 基于 DOM 的

![alt text](DOM内容展示.png)

早期的 GUI Agent 受限于 LLM 只能处理文本输入的局限性。因此，这些代理依赖于将GUI页面转换为结构化的文本表示，例如HTML、可访问性树或文档对象模型（DOM）。

然而，DOM的缺点是内容过分冗余。比如，12306.cn主页的DOM文本消耗高达90万tokens。即使对DOM内容进行过滤裁剪，也需要消耗20万tokens。相比而言，一张截图仅消耗1000 tokens左右。

此外，DOM是不完备的。它不是网页的最终呈现，还受到css文件、业务逻辑js代码的很大影响。

比如，我要收集操作轨迹的训练数据。如果依赖截图，完全可以在互联网电脑上收集截图及标注员的操作，拷贝到内网标注平台进行标注。但是如果收集DOM数据，缺少了CSS文件及业务逻辑代码文件，无法在其他设备上还原网页的呈现内容。

### 基于屏幕截图的

难点：GUI图像的信息密度和结构性通常比通用场景图像更高，往往包含数百个排列成复杂布局的元素。模型不仅要识别单个元素，还要理解它们的空间关系和功能交互。此外，GUI图像中的许多元素都很小（例如，在 $1920 \times 1080$ 的屏幕截图中的一个 $10 \times 10$ 像素的图标），这使得准确感知和定位这些元素变得困难。

为了解决这一问题，我们需要构建专门的GUI数据集和训练任务，对基线模型进行微调，从而提高大模型对于屏幕截图中元素感知与定位的准确性。

## 思考与行动

大模型将屏幕截图作为观测输入，首先输出思考内容，例如网页内容或当前状态，以及下一步要采取的行动。然后，根据这些思考内容，模型输出一个以工具调用形式表示的行动。

![alt text](image-5.png)

例如，用户希望 GUI Agent 自动执行任务：在 GitHub 网站上购买 Copilot 增值服务。

大模型首先观察到的是浏览器的初始界面。其思考内容是：应该导航至 GitHub Copilot 产品购买界面，然后执行对应动作。

接下来，大模型观察到已经进入到了指定页面，其思考内容是：应该点击“查看订阅计划及价格”链接。执行的动作为，点击该链接对应的坐标。

GUI Agent 能够执行的全部动作列举如下：

| 动作       | 描述                                   |
| :--------- | :------------------------------------- |
| 按键       | 按指定顺序按下按键（例如 *CTRL+C*）。  |
| 输入       | 在坐标 $(x, y)$ 处输入字符串。         |
| 移动鼠标   | 移动光标悬停在坐标 $(x, y)$ 上。       |
| 左键单击   | 在坐标 $(x, y)$ 处单击鼠标左键。       |
| 滚动       | 滚动鼠标滚轮。                         |
| 访问链接   | 访问指定的 URL。                       |
| 网络搜索   | 使用指定查询进行网络搜索。             |
| 历史回退   | 返回上一页。                           |
| 记忆       | 记忆信息以供将来参考。                 |
| 等待       | 等待指定的秒数。                       |
| 终止       | 结束当前任务。                         |

# 训练数据集构建与模型训练

未针对流程自动化任务训练的通用模型存在如下缺点：
- 屏幕截图属于领域性很强的图像，且要定位的元素很小，通用模型表现不佳。
- 完成流程自动化任务需要领域专门知识。比如，如果大模型不知道 GitHub Copilot 的产品购买页面对应的网址，便无法完整相关流程自动化任务。

为此，需要设计多种训练任务，通过对通用模型的微调，实现流程自动化的能力。

#### 轨迹任务

将每条轨迹的每一步单独作为一个训练样本，把截至当前步骤的历史观测与动作作为模型输入。

我们采用 Qwen2.5-VL 的定位规范，并预测绝对坐标。我们使用标准的交叉熵损失，所有输出都是模型词表中的 token，包括坐标。

#### UI问答任务

一条训练数据包括三部分：
- 截图
- 问题
- 回答

![alt text](image-3.png)

问题：门票的配送日期是？

回答：2025 年 3 月 28 日

#### UI定位任务

一条训练数据包括三部分：
- 截图
- 问题
- 回答

![alt text](image-4.png)

问题：点击衣服的 XL 尺寸

回答：click(1189, 252)

### 训练细节

我们使用 Qwen2.5-VL-7B 作为基座模型，并在其基础上进行监督微调（SFT）。

我们使用标准的交叉熵损失，所有输出都是模型词表中的 token，包括坐标。

把每一步单独切成一条训练数据。

对于轨迹数据，我们将每条轨迹的每一步单独作为一个训练样本，把截至当前步骤的历史观测与动作作为模型输入。

- 优化器：AdamW，其参数设置为 $\beta_1 = 0.9, \beta_2 = 0.95$
- 学习率：在训练步数的前 10% 阶段采用余弦学习率预热策略；预热结束后，初始学习率设置为 $5\times 10^{-6}$
- 梯度裁剪：最大阈值设为 1
- 批次大小 = 128
- 参数精度：BF16

## 基线模型的选择

微软与2025年底提出的 Fara-7B，以 Qwen2.5-VL-7B 作为基座模型，并在其基础上进行监督微调（SFT），微调样本量为 180 万。

![alt text](fara7b训练数据.png)

训练对于模型处理GUI自动化任务具有关键作用。

![alt text](训练的作用.png)

接下来，我们要在公安领域收集训练数据，进一步微调该模型。

具体而言，对于给定的公安业务系统，用户定义若干自动化任务。比如，查询xxx的身份证号、查询xxx车牌的轨迹。先让基线模型执行该自动化任务。模型可能执行正确，也可能执行错误。

这时可以计算基线模型在给定业务平台上的成功率。

对于出错的任务，用户给出正确操作方式，作为一条训练数据。

收集到训练数据后，便可进行模型微调。

下一步计划：

1. 在给定的公安业务系统上，测试基线模型的准确率
2. 对于出错任务，收集训练数据
3. 微调模型，提升准确率。

